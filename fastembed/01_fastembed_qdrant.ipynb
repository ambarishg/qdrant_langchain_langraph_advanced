{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b50b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ambarish\\qdrant_examples\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\":memory:\")  # Qdrant is running from RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c05bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Qdrant has a LangChain integration for chatbots.\",\n",
    "    \"Qdrant has a LlamaIndex integration for agents.\",\n",
    "]\n",
    "metadata = [\n",
    "    {\"source\": \"langchain-docs\"},\n",
    "    {\"source\": \"llamaindex-docs\"},\n",
    "]\n",
    "ids = [42, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac84b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "client.create_collection(\n",
    "    collection_name=\"test_collection\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=client.get_embedding_size(model_name), \n",
    "        distance=models.Distance.COSINE\n",
    "    ),  # size and distance are model dependent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e719ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Ambarish\\qdrant_examples\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ambarish Ganguly\\AppData\\Local\\Temp\\fastembed_cache\\models--Qdrant--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 5 files:  40%|████      | 2/5 [00:11<00:17,  5.89s/it]\n",
      "\u001b[32m2025-08-24 19:23:08.065\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m430\u001b[0m - \u001b[31m\u001b[1mCould not download model from HuggingFace: [WinError 1314] A required privilege is not held by the client: '..\\\\..\\\\blobs\\\\a8b3208c2884c4efb86e49300fdd3dc877220cdf' -> 'C:\\\\Users\\\\Ambarish Ganguly\\\\AppData\\\\Local\\\\Temp\\\\fastembed_cache\\\\models--Qdrant--bge-small-en\\\\snapshots\\\\8791246cc2a79c7949a4dc0d4a018cbd7d024879\\\\special_tokens_map.json' Falling back to other sources.\u001b[0m\n",
      "100%|██████████| 77.7M/77.7M [00:07<00:00, 9.82MiB/s]\n"
     ]
    }
   ],
   "source": [
    "metadata_with_docs = [\n",
    "    {\"document\": doc, \"source\": meta[\"source\"]} for doc, meta in zip(docs, metadata)\n",
    "]\n",
    "client.upload_collection(\n",
    "    collection_name=\"test_collection\",\n",
    "    vectors=[models.Document(text=doc, model=model_name) for doc in docs],\n",
    "    payload=metadata_with_docs,\n",
    "    ids=ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0d0076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoredPoint(id=2, version=0, score=0.8749180307571953, payload={'document': 'Qdrant has a LlamaIndex integration for agents.', 'source': 'llamaindex-docs'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=42, version=0, score=0.8351846658199004, payload={'document': 'Qdrant has a LangChain integration for chatbots.', 'source': 'langchain-docs'}, vector=None, shard_key=None, order_value=None)]\n"
     ]
    }
   ],
   "source": [
    "search_result = client.query_points(\n",
    "    collection_name=\"test_collection\",\n",
    "    query=models.Document(\n",
    "        text=\"Which integration is best for agents?\", \n",
    "        model=model_name\n",
    "    )\n",
    ").points\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e5d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
