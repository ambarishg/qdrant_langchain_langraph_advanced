{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bf640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395e116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "    # we are using gemini model. You can use different models.\n",
    "    from langchain.chat_models import init_chat_model\n",
    "    from dotenv import load_dotenv  # used to store secret stuff like API keys or configuration values\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    llm = init_chat_model(\n",
    "        \"azure_openai:gpt-4o\",\n",
    "        azure_deployment=\"gpt4o\",\n",
    "    )\n",
    "    metadata = f\"CRAG, gpt4o\"\n",
    "    return llm, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2374ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, metadata = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ee1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"voltage\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"\n",
    "        \n",
    "        Given a user question choose to route it to \n",
    "        Voltage and Roadways store  or web search.\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"\n",
    "You are an expert at routing a user question to a \n",
    "Voltage and Roadways store or web search.\n",
    "Voltage and Roadways store has information about medium voltage and road ways.\n",
    "Web search has information about current events and news.\n",
    "You must choose the most relevant datasource to answer the question.\n",
    "\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb159db",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "collection_name = \"PWD_SENTENCE_TRANSFORMERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c973d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ambarish\\qdrant_langchain_langraph_advanced\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7bf12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_name = MODEL_NAME\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52eec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    url=url,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0126994",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever  = qdrant.as_retriever(search_type=\"similarity\", \n",
    "                                 search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bc33c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ambarish\\qdrant_langchain_langraph_advanced\\.venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f457f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "203d8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bcd345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_generate(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents and generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New keys added to state, documents and generation, that contains retrieved documents and LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE AND GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    # Retrieval\n",
    "    docs = retriever.invoke(question)\n",
    "    documents = \"\\n\".join([d.page_content for d in docs])\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd7b2381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ambarish Ganguly\\AppData\\Local\\Temp\\ipykernel_13680\\730760015.py:5: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(k=3)\n"
     ]
    }
   ],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bebc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_and_generate(state):\n",
    "    \"\"\"\n",
    "    Web search and generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New keys added to state, documents and generation, that contains retrieved documents and LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---WEB SEARCH AND GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": [web_results], \"question\": question})\n",
    "    return {\"documents\": [web_results], \"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cd25ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. \n",
    "    'No' means that the answer does not resolve the question.\n",
    "    Yes' means that the answer resolves the question.\n",
    "     \"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e923a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_answer(state):\n",
    "    \"\"\"\n",
    "    Grade the answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, grade, that contains binary score 'yes' or 'no'\n",
    "    \"\"\"\n",
    "    print(\"---GRADE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = state[\"generation\"]\n",
    "    grade = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "    print(f\"Grade: {grade.binary_score}\")\n",
    "    return grade.binary_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ff0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"voltage\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"voltage\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acadbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search\", web_and_generate)  # web search\n",
    "workflow.add_node(\"voltage\", retrieve_and_generate)  # retrieve\n",
    "\n",
    "workflow.add_edge(START, \"voltage\")\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\"voltage\",\n",
    "                                grade_answer, \n",
    "                                {\"yes\": END, \n",
    "                                 \"no\": \"web_search\"\n",
    "                                 })\n",
    "workflow.add_edge(\"web_search\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d4a6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is LLAMA 4?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d9f628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Mechanical busbar strength and formula ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebfc68de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE AND GENERATE---\n",
      "---GRADE ANSWER---\n",
      "Grade: yes\n",
      "\"Node 'voltage':\"\n",
      "'------------------'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Mechanical busbar strength refers to the capacity of busbars to withstand mechanical stresses without failure, particularly focusing on the stress resulting from bending moments. The formula to calculate the resultant stress is: η = (F1 × l) / (12 × I), where η is the resultant stress in daN/cm², F1 is the force, l is the distance between insulators, and I is the modulus of inertia. In the provided context, an example calculation resulted in a stress η of 195 daN/cm²."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mechanical busbar strength refers to the capacity of busbars to withstand '\n",
      " 'mechanical stresses without failure, particularly focusing on the stress '\n",
      " 'resulting from bending moments. The formula to calculate the resultant '\n",
      " 'stress is: η = (F1 × l) / (12 × I), where η is the resultant stress in '\n",
      " 'daN/cm², F1 is the force, l is the distance between insulators, and I is the '\n",
      " 'modulus of inertia. In the provided context, an example calculation resulted '\n",
      " 'in a stress η of 195 daN/cm².')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": question}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        #pprint(value[\"generation\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"------------------\")\n",
    "\n",
    "# Final generation\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(value[\"generation\"]))\n",
    "\n",
    "pprint(value[\"generation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90df14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
